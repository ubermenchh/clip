# clip

[paper](https://arxiv.org/abs/2103.00020)
[](https://ar5iv.labs.arxiv.org/html/2103.00020/assets/x1.png)

simple implementation of CLIP (Contrastive Language-Image Pre-training) in PyTorch

References:
- [Official OpenAI repo](https://github.com/openai/CLIP)
- [CLIP_FOR_MNIST](https://github.com/tianlinxu312/Everything-about-LLMs/blob/main/CLIP_for_MNIST.ipynb)
